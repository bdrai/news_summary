{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d270e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:16:37.013122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faf96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('train.csv', nrows=1000)\n",
    "\n",
    "# Preprocess the articles and highlights\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9.,!?%$ ]', '', text)\n",
    "    return text\n",
    "\n",
    "data['article'] = data['article'].apply(preprocess_text)\n",
    "data['highlights'] = data['highlights'].apply(preprocess_text)\n",
    "data['highlights'] = data['highlights'].apply(lambda x: '<sos> ' + x + ' <eos>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c9e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the articles and highlights\n",
    "article_tokenizer = Tokenizer()\n",
    "article_tokenizer.fit_on_texts(data['article'])\n",
    "article_sequences = article_tokenizer.texts_to_sequences(data['article'])\n",
    "\n",
    "highlight_tokenizer = Tokenizer()\n",
    "highlight_tokenizer.fit_on_texts(data['highlights'])\n",
    "highlight_sequences = highlight_tokenizer.texts_to_sequences(data['highlights'])\n",
    "\n",
    "max_article_len = max([len(seq) for seq in article_sequences])\n",
    "max_highlight_len = max([len(seq) for seq in highlight_sequences])\n",
    "\n",
    "article_sequences_padded = pad_sequences(article_sequences, maxlen=max_article_len)\n",
    "highlight_sequences_padded = pad_sequences(highlight_sequences, maxlen=max_highlight_len)\n",
    "\n",
    "# Create input and target sequences\n",
    "X = article_sequences_padded\n",
    "y = highlight_sequences_padded[:, :-1]\n",
    "y_target = highlight_sequences_padded[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88caae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:16:39.782800: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_article_len,))\n",
    "encoder_embedding = Embedding(len(article_tokenizer.word_index) + 1, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(hidden_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(len(highlight_tokenizer.word_index) + 1, embedding_dim)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(hidden_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(len(highlight_tokenizer.word_index) + 1, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3639530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 111s 8s/step - loss: 7.8305 - val_loss: 4.5342\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 115s 9s/step - loss: 2.7092 - val_loss: 1.8216\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 109s 8s/step - loss: 2.0652 - val_loss: 1.8584\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 109s 8s/step - loss: 1.9750 - val_loss: 1.8360\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 109s 8s/step - loss: 1.9311 - val_loss: 1.8103\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit([X, y], np.expand_dims(y_target, -1),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4a8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(hidden_dim,))\n",
    "decoder_state_input_c = Input(shape=(hidden_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedded_inference = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs_inference, state_h_inference, state_c_inference = decoder_lstm(decoder_embedded_inference, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4054ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552    cnn  spains controversial, and highly restrict...\n",
      "Name: article, dtype: object\n",
      "1/1 [==============================] - 0s 103ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<sos>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m sample_article \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(data\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample_article)\n\u001b[0;32m---> 35\u001b[0m generated_summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_article\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_summary)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mgenerate_summary\u001b[0;34m(article_text)\u001b[0m\n\u001b[1;32m     12\u001b[0m encoder_states_value \u001b[38;5;241m=\u001b[39m encoder_model\u001b[38;5;241m.\u001b[39mpredict(article_sequence_padded)\n\u001b[1;32m     14\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mhighlight_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<sos>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m summary \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '<sos>'"
     ]
    }
   ],
   "source": [
    "# Create the inference decoder model\n",
    "decoder_states_inference = [state_h_inference, state_c_inference]\n",
    "decoder_outputs_inference = decoder_dense(decoder_outputs_inference)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs_inference] + decoder_states_inference)\n",
    "\n",
    "def generate_summary(article_text):\n",
    "    article_input = preprocess_text(article_text)\n",
    "    article_sequence = article_tokenizer.texts_to_sequences([article_input])\n",
    "    article_sequence_padded = pad_sequences(article_sequence, maxlen=max_article_len)\n",
    "\n",
    "    encoder_states_value = encoder_model.predict(article_sequence_padded)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = highlight_tokenizer.word_index['<sos>']\n",
    "    summary = []\n",
    "\n",
    "    while True:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + encoder_states_value)\n",
    "        token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        token = highlight_tokenizer.index_word[token_index]\n",
    "\n",
    "        if token == '<eos>' or len(summary) >= max_highlight_len:\n",
    "            break\n",
    "\n",
    "        summary.append(token)\n",
    "        target_seq[0, 0] = token_index\n",
    "        encoder_states_value = [h, c]\n",
    "\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Test the model on a sample article\n",
    "sample_article = str(data.sample(1)['article'])\n",
    "print(sample_article)\n",
    "generated_summary = generate_summary(sample_article)\n",
    "print(\"Generated summary:\", generated_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4da8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d53c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da012b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e89da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
